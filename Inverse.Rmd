---
title: "Inverse"
author: "Suhasini Subba Rao"
date: "2025-05-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


Consider the nonstationary AR model 
\begin{eqnarray*}
\left(
\begin{array}{c}
X_{t}^{(1)} \\
X_{t}^{(2)} \\
X_{t}^{(3)} \\
\end{array}
\right)=
X_{t} = 
\left(
\begin{array}{ccc}
\alpha_1(t) & 0.2 & 0 \\
\alpha_2(t)  & 0.8 & 0 \\
0 & 0.3 & 0.6 \\
 \end{array}
\right)X_{t-1}+\varepsilon_{t} = A(t)X_{t-1} + \varepsilon_{t}
\end{eqnarray*}
with $\varepsilon_{t}\sim N(0,I_3)$ and $X_{1}=\varepsilon_{1}$.
If $\alpha(t)$ is constant over time then $X_{t}$ is a stationary time series. Else the time-varying $\alpha(t)$ will mean that 
all the three time series $\{X_{t}^{(a)}\}$, $\{X_{t}^{(b)}\}$
and $\{X_{t}^{(c)}\}$ are nonstationary in the sense the autocovariance changes over time where
\begin{eqnarray*}
C_{t,\tau} = \sum_{j=0}^{t}\left[\prod_{i=0}^{j}A(t-i)\right]^{\top}
\left[\prod_{i=0}^{j+|\tau-t|}A(\tau-i)\right] \qquad \tau\geq t
\end{eqnarray*}

Let ${\bf X}_{n} = (X_{1},\ldots,X_{n})^{\top}$ and 
$C_{n} = var({\bf X}_{n})$. Then 
\begin{eqnarray*}
D_n = L_n^{\top}L_n
\end{eqnarray*}
where 
\begin{eqnarray*}
L_n = \left(
\begin{array}{ccccc}
I & 0 & 0 & \ldots & 0 \\
-A(2) & I & 0 & \ldots  &0 \\
0 & -A(3) & I & \ldots  &0 \\
\vdots &  \vdots & \vdots & \ddots & 0 \\
0 & 0 & 0 & \ldots &  I \\
\end{array}
\right)
\end{eqnarray*}
We see from above that directly calculating $C_{t,\tau}$ can be difficult, but they obtained by inverting $D_{n}$ which is easier to calculate. 

The code below evaluates $D_n$ and $C_n$ for a given $\{A(t)\}$.
```{r}
n=100 # This sample size is used throughout
# Here we contruct D and C for the above model. 
# Stationary model
DD1 = matrix(rep(0,(3*n)**2),ncol = 3*n)
# A corresponds to the transition matrix of a VAR(1)
A =  matrix(c(0.5,0.2,0,-0.3,0.6,0,0,0.3,0.6),ncol=3,byrow=T) 
Id = matrix(c(1,0,0,0,1,0,0,0,1),ncol=3)
st =  rep(0.7,n)
# Keep st constant if we want process to be stationary
# First construct my upper triangular matrix L_{n}^{\top}
for(tt in c(1:(n-1))){
        A[1,1] = st[tt] # change first line to tv parameter
        # need to ensure spectral radius of A is less than one
        ccord = c((3*(tt-1)+1):(3*tt))
        ccord1 = c((3*tt+1):(3*(tt+1))) 
        DD1[ccord,ccord] = Id
        DD1[ccord,ccord1] = -t(A)         
            }
       ccord = c((3*(n-1)+1):(3*n))
       DD1[ccord,ccord] = Id
       
# using the upper triangular matrix construct D       
DDsttemp = DD1%*%t(DD1)
# This adjusts the edge of the matrix to make it stationary (do not worry about it :)
DDsttemp[c(1:3),c(1:3)] = Id
#DDsttemp[c(1:6),c(1:15)]
CCsttemp  = solve(DDsttemp)
#CCsttemp 
seq1 = seq(1,3*n,by=3)
seq2 = seq(2,3*n,by=3)
seq3 = seq(3,3*n,by=3)
CCsttemp11 = CCsttemp[seq1,seq1]
CCsttemp22 = CCsttemp[seq2,seq2]
CCsttemp33 = CCsttemp[seq3,seq3]
CCsttemp21 = CCsttemp[seq2,seq1]
CCsttemp31 = CCsttemp[seq3,seq1]
CCsttemp23 = CCsttemp[seq2,seq3]
DDsttemp11 = DDsttemp[seq1,seq1]
DDsttemp22 = DDsttemp[seq2,seq2]
DDsttemp33 = DDsttemp[seq3,seq3]
DDsttemp21 = DDsttemp[seq2,seq1]
DDsttemp31 = DDsttemp[seq3,seq1]
DDsttemp23 = DDsttemp[seq2,seq3]
```

```{r}
# nonstationary model
DD2 = matrix(rep(0,(3*n)**2),ncol = 3*n)
A =  matrix(c(0.5,0.2,0,0,0.8,0,0,0.3,0.6),ncol=3,byrow=T)
Id = matrix(c(1,0,0,0,1,0,0,0,1),ncol=3)
st =  0.3+0.5*(1+exp(0.05*(c(1:n)-(n/2))))**(-1)
st2 = -0.2-0.5*(1+exp(0.05*(c(1:n)-(n/2))))**(-1)
for(tt in c(1:(n-1))){
        A[1,1] = st[tt] # change first line to tv parameter
        A[2,1] = st2[tt]
  # need to ensure spectral radius of A is less than one      
        ccord = c((3*(tt-1)+1):(3*tt))
        ccord1 = c((3*tt+1):(3*(tt+1))) 
        DD2[ccord,ccord] = Id
        DD2[ccord,ccord1] = -t(A)         
            }
       ccord = c((3*(n-1)+1):(3*n))
       DD2[ccord,ccord] = Id
DD2[c(1:9),c(1:9)]
       
DDnsttemp = DD2%*%t(DD2)
CCnsttemp  = solve(DDnsttemp)
seq1 = seq(1,3*n,by=3)
seq2 = seq(2,3*n,by=3)
seq3 = seq(3,3*n,by=3)
CCnsttemp11 = CCnsttemp[seq1,seq1]
CCnsttemp22 = CCnsttemp[seq2,seq2]
CCnsttemp33 = CCnsttemp[seq3,seq3]
CCnsttemp21 = CCnsttemp[seq2,seq1]
CCnsttemp31 = CCnsttemp[seq3,seq1]
CCnsttemp23 = CCnsttemp[seq2,seq3]
DDnsttemp11 = DDnsttemp[seq1,seq1]
DDnsttemp22 = DDnsttemp[seq2,seq2]
DDnsttemp33 = DDnsttemp[seq3,seq3]
DDnsttemp21 = DDnsttemp[seq2,seq1]
DDnsttemp31 = DDnsttemp[seq3,seq1]
DDnsttemp23 = DDnsttemp[seq2,seq3]
```
Above we permute the entries of the matrix of $C_{n}$ and $D_{n}$ 
such that are grouped according to time series $a,b,c$.
\begin{eqnarray*}
\widetilde{C}_{n} = \left(
\begin{array}{ccc}
C_{n}^{(a,a)} & C_{n}^{(a,b)} & C_{n}^{(a,c)} \\
C_{n}^{(b,a)} & C_{n}^{(b,b)} & C_{n}^{(b,c)} \\
C_{n}^{(c,a)} & C_{n}^{(c,b)} & C_{n}^{(c,c)} \\
\end{array}
\right), \quad
\widetilde{D}_{n} = \left(
\begin{array}{ccc}
D_{n}^{(a,a)} & D_{n}^{(a,b)} & D_{n}^{(a,c)} \\
D_{n}^{(b,a)} & D_{n}^{(b,b)} & D_{n}^{(b,c)} \\
D_{n}^{(c,a)} & D_{n}^{(c,b)} & D_{n}^{(c,c)} \\
\end{array}
\right).
\end{eqnarray*}
$C_{n}^{(a,a)}$ is an $n\times n$ matrix which contains all the covariance $cov[X_{t}^{(a)},X_{\tau}^{(b)}]$ for $t,\tau\in \{1,\ldots,n\}$. If $\{X_{t}^{(a)}\}$ is stationary then 
$cov[X_{t}^{(a)},X_{t+r}^{(b)}]$ is the same for all $r$. 
We can check this using the above example (for the stationary AR model); note that there will be some differences close to the edge of the matrix. 
```{r}
# plot a diagonal or off-diagonal
offdiag0 = rep(0,n)
for(i in c(1:n)){
  offdiag0[i] = CCsttemp11[i,i]
}
r = 1
offdiagr = rep(0,(n-r))
for(i in c(1:(n-r))){
  offdiagr[i] = CCsttemp11[i,(i+r)]
}
# First for C (along the diagonal) self
offdiag011 = rep(0,n)
for(i in c(1:n)){
  offdiag011[i] = CCsttemp11[i,i]
}
offdiag022 = rep(0,n)
for(i in c(1:n)){
  offdiag022[i] = CCsttemp22[i,i]
}
offdiag033 = rep(0,n)
for(i in c(1:n)){
  offdiag033[i] = CCsttemp33[i,i]
}
par(mfrow = c(1,3))
plot.ts(offdiag011)
plot.ts(offdiag022)
plot.ts(offdiag033)
```
The plots are not completely constant because the inverse $C_{n}$ is subject to numerical approximation errors (I think), which is frustrating.  
But what you do see is that overall the variance is constant over time. You will see the same is true for all the covariances at different lags.
(try it).


```{r}
# To see the covariance decay plot along one row too
plot.ts(CCnsttemp11[1,])
```
Now we look at the nonstationary case.
```{r}
# First for C (along the diagonal) with itself
offdiag011 = rep(0,n)
for(i in c(1:n)){
  offdiag011[i] = CCnsttemp11[i,i]
}
offdiag011[c(1:100)]
offdiag022 = rep(0,n)
for(i in c(1:n)){
  offdiag022[i] = CCnsttemp22[i,i]
}
offdiag022[c(1:100)]
#CCnsttemp22[c(10:30),c(10:30)]
offdiag033 = rep(0,n)
for(i in c(1:n)){
  offdiag033[i] = CCnsttemp33[i,i]
}
par(mfrow = c(1,3))
plot.ts(offdiag011)
plot.ts(offdiag022)
plot.ts(offdiag033)
# C (along the diagonal) with others
offdiag021 = rep(0,n)
for(i in c(1:n)){
  offdiag021[i] = CCnsttemp21[i,i]
}
offdiag023 = rep(0,n)
for(i in c(1:n)){
  offdiag023[i] = CCnsttemp23[i,i]
}
offdiag031 = rep(0,n)
for(i in c(1:n)){
  offdiag031[i] = CCnsttemp31[i,i]
}
par(mfrow = c(1,3))
plot.ts(offdiag021)
plot.ts(offdiag023)
plot.ts(offdiag031)

# Next for D (along the diagonal) nonstationary case with itself
offdiag011 = rep(0,n)
for(i in c(1:n)){
  offdiag011[i] = DDnsttemp11[i,i]
}
offdiag022 = rep(0,n)
for(i in c(1:n)){
  offdiag022[i] = DDnsttemp22[i,i]
}
offdiag033 = rep(0,n)
for(i in c(1:n)){
  offdiag033[i] = DDnsttemp33[i,i]
}
par(mfrow = c(1,3))
plot.ts(offdiag011)
plot.ts(offdiag022)
plot.ts(offdiag033)
# For D (along the diagonal) with others
offdiag021 = rep(0,n)
for(i in c(1:n)){
  offdiag021[i] = DDnsttemp21[i,i]
}
offdiag023 = rep(0,n)
for(i in c(1:n)){
  offdiag023[i] = DDnsttemp23[i,i]
}
offdiag031 = rep(0,n)
for(i in c(1:n)){
  offdiag031[i] = DDnsttemp31[i,i]
}
par(mfrow = c(1,3))
plot.ts(offdiag021)
plot.ts(offdiag023)
plot.ts(offdiag031)
```
What do you observe in the plots? How are $C$ and $D$ different. We have learnt that 
$D$ contain information on the conditional dependence. Hence a zero matrix
$D_{n}^{(a,b)}$ means that $\{X_{t}^{(a)}\}$ and $\{X_{t}^{(b)}\}$ have no partial correlation. But for nonstationary time series even non-zero matrices 
$D_{n}^{(a,b)}$ hold interesting information about the conditional 
dependence between $\{X_{t}^{(a)}\}$ and $\{X_{t}^{(b)}\}$. Indeed if 
$D_{n}^{(a,b)}$ (including the case $a=b$) is a Toeplitz matrix (or Toeplitz if we ignore the edge), then it means that after conditioning the time series becomes stationary even though the time series itself is nonstationary. This can lead to fascinating insights about the multivariate nonstationary time series. 
\begin{itemize}
\item The time series $\{X_{t}\}$ is nonstationary, hence most if not all the covariance matrices $C_{n}^{(a,b)}$ are not Toeplitz (constant along the diagonal). But the inverse matrices $D_{n}^{(a,b)}$ can still be zero or Toeplitz. If $D_{n}^{(a,b)}$ is a 
zero matrix then there is no partial correlation between time series $a$ and $b$. On the other hand, if the matrix 
$D_{n}^{(a,a)}$ is Toeplitz then time series $\{X_{t}^{(a)}\}$ is conditionally stationary even if the time series itself is nonstationary.
\end{itemize}

The question is how to go about detecting this behaviour in practice. To do that 
we first map our time series to the Fourier domain. 


\section{Mapping into the Fourier domain}



We require the use of the DFT matrix $F_{n} = (\exp(2\pi i(k_1-k_2)/\sqrt{n};k_1,k_2\in \{1,\ldots,n\})$
```{r}
fftmatrix = function(n){
    w = rep(0,n)
    w[2] = 1
    u = fft(w)
    Mat = matrix(rep(0,n^2),ncol = n)
    for(j in c(1:n)) Mat[j,] = u^j
    Mat1 = Mat/sqrt(n)
    return(Mat1)
    }
```

\subsection{Fourier transform of the covariance}

We consider the covariance between the DFTs of the stationary and nonstationary 
autoregressive models, and compare the two. 
Note that if 
${\bf J}_{n}^{(a)} = (J_{k,n}^{(a)};k=1,\ldots,n)$ with
$J_{k,n}^{(a)} = \frac{1}{n^{1/2}}\sum_{t=1}^{n}X_{t}^{(a)}\exp(it\omega_k)$ and $\omega_{k} = 2\pi k/n$. Then 
\begin{eqnarray*}
cov[{\bf J}_{n}^{(a)},{\bf J}_{n}^{(b)}] = F_{n}^{*}C_{n}^{(a,b)}F_{n}
\end{eqnarray*}
where $F_{n}$ is the FFT matrix (defined in the Rcode above).
Below we make plots of rows of this matrix in the stationary and nonstationary cases. 
```{r}
FF = fftmatrix(n)
FFstar = t(Conj(FF))
FCst11 = FF%*%CCsttemp11%*%FFstar
FCst22 = FF%*%CCsttemp22%*%FFstar
FCst33 = FF%*%CCsttemp33%*%FFstar
FCst21 = FF%*%CCsttemp21%*%FFstar
FCst31 = FF%*%CCsttemp31%*%FFstar
FCst23 = FF%*%CCsttemp23%*%FFstar
par(mfrow=c(2,1))
plot.ts(abs(FCst11[1,]))
plot.ts(abs(FCst22[1,]))
abs(FCst22[1,c(1:20)])
par(mfrow=c(2,1))
plot.ts(abs(FCst11[30,]))
plot.ts(abs(FCst22[30,]))
abs(FCst22[30,c(20:40)])
```
Now we do the same for the nonstationary time series. 
```{r}
FCnst11 = FF%*%CCnsttemp11%*%FFstar
FCnst22 = FF%*%CCnsttemp22%*%FFstar
FCnst33 = FF%*%CCnsttemp33%*%FFstar
FCnst21 = FF%*%CCnsttemp21%*%FFstar
FCnst31 = FF%*%CCnsttemp31%*%FFstar
FCnst23 = FF%*%CCnsttemp23%*%FFstar
par(mfrow=c(2,1))
plot.ts(abs(FCnst11[1,]))
plot.ts(abs(FCnst22[1,]))
abs(FCnst22[1,c(1:20)])
par(mfrow=c(2,1))
plot.ts(abs(FCnst11[30,]))
plot.ts(abs(FCnst22[30,]))
abs(FCnst11[30,c(20:40)])
abs(FCnst22[30,c(20:40)])
```
The difference between the stationary and nonstationary cases are subtle but there. 
What you should observe is that for the stationary time series the variance of the DFT is 
extremely large but the covariance between the DFTs even at freqeuency lags that are extremely small 
is very very small as compared to the variance. 
On the other hand, for nonstationary VAR there is clear correlation at low lags, even though it smaller than the variance. 


\subsection{Fourier transform of the inverse covariance}

Now we consider the inverse covariance in the nonstationary case. This corresponds to 
\begin{eqnarray*}
\left(var[{\bf J}_{n}]\right)^{-1} = F_{n}^{*}D_{n}F_{n}.
\end{eqnarray*}
where ${\bf J}_n = ({\bf J}_{n}^{(1)},{\bf J}_{n}^{(2)},{\bf J}_{n}^{(3)})$.
We look at the permuted version of this matrix. This is given below. 
```{r}
FDnst11 = FF%*%DDnsttemp11%*%FFstar
FDnst22 = FF%*%DDnsttemp22%*%FFstar
FDnst33 = FF%*%DDnsttemp33%*%FFstar
FDnst21 = FF%*%DDnsttemp21%*%FFstar
FDnst31 = FF%*%DDnsttemp31%*%FFstar
FDnst23 = FF%*%DDnsttemp23%*%FFstar
par(mfrow=c(1,3))
plot.ts(abs(FDnst11[50,]))
plot.ts(abs(FDnst22[50,]))
plot.ts(abs(FDnst33[50,]))
abs(FDnst11[50,c(40:60)])
abs(FDnst22[50,c(40:60)])
abs(FDnst33[50,c(40:60)])
# Do the same but for the cross inverse covariances 
par(mfrow=c(1,3))
plot.ts(abs(FDnst21[50,]))
plot.ts(abs(FDnst31[50,]))
plot.ts(abs(FDnst23[50,]))
abs(FDnst21[50,c(40:60)])
abs(FDnst31[50,c(40:60)])
abs(FDnst23[50,c(40:60)])
```
Observe that at frequency $\omega_{k} = \pi$ (corresponds to $k=50$ in a sample size of $n=100$) 
there is a subtle difference in the plots. 
The Fourier transform of $D_{n}^{(2,2)}$ and $D_{n}^{(3,3)}$ are "almost" zero everywhere 
except at the frequency $k=50$, whereas the plots of the 
Fourier transform of $D_{n}^{(1,1)}$ shows some slightly larger values close to 
frequency $k=50$. This is because in the plots above $D_{n}^{(1,1)}$ is not constant along the sub-diagonals 
(it is not Toeplitz). However, this difference is not so clearly seen at all frequencies. 

\subsection{Inverse of smaller matrices}

In practice given data directly estimating $\left(var[{\bf J}_{n}]\right)^{-1}$ will not be possible as the dimension 
(without some form of regularisation). For small $p$ (where $p$ denotes the dimension of the matrix) one could look at
${\bf J}_k^{\nu} =
 (J_{k-\nu}^{\top},J_{k-\nu+1}^{\top},\ldots,J_{k}^{\top},\ldots,J_{k+\nu}^{\top})$ where 
 $J_{k} = (J_{k}^{(1)},\ldots,J_{k}^{(p)})$ and evaluate 
 $(var[{\bf J}_k^{\nu}])^{-1}$. In the below we set $k=50$ and $\nu=5$
```{r}
k=50
nu=3 
seqk = c((k-nu):(k+nu)) # extracts frequency k-nu to k+nu 
FCnst11k = FCnst11[seqk,seqk] 
FCnst22k = FCnst22[seqk,seqk] 
FCnst33k = FCnst33[seqk,seqk] 
FCnst21k = FCnst21[seqk,seqk] 
FCnst31k = FCnst31[seqk,seqk] 
FCnst23k = FCnst23[seqk,seqk] 
FCkrow1 = cbind(FCnst11k,t(Conj(FCnst21k)),t(Conj(FCnst31k))) 
FCkrow2 = cbind(FCnst21k,FCnst22k,FCnst23k)
FCkrow3 = cbind(FCnst31k,t(Conj(FCnst23k)),FCnst33k)
# FCk is the covariance matrix of the DFTs from k-nu to k+nu.
FCk = rbind(FCkrow1,FCkrow2,FCkrow3)
FDnuk = solve(FCk)
seq1 = c(1:7)
seq2 = c(8:14)
seq3 = c(15:21)
FDnst11knu = FDnuk[seq1,seq1]
FDnst22knu = FDnuk[seq2,seq2]
FDnst33knu = FDnuk[seq3,seq3]
FDnst21knu = FDnuk[seq2,seq1]
FDnst31knu = FDnuk[seq3,seq1]
FDnst23knu = FDnuk[seq2,seq3]
```
Below we compare the inverse matrix about frequency k with the approximate
inverse matrix centered about frequency k.
```{r}
abs(FDnst11[50,seqk])
abs(FDnst11knu[4,]) # The approximation
abs(FDnst22[50,seqk])
abs(FDnst22knu[4,]) # The approximation
abs(FDnst33[50,seqk])
abs(FDnst33knu[4,]) # The approximation
```
 
 
 



